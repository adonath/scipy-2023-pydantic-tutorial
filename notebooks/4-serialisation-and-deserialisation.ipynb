{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5326c889",
   "metadata": {},
   "source": [
    "# Serialization and Deserialization\n",
    "\n",
    "**Concepts:**\n",
    "* Motivation - Need for serialization and deserialization of Python objects\n",
    "* Introduce JSON / YAML / TOML formats\n",
    "* Serialize Pydantic models to from JSON / YAML\n",
    "* Deserialize Pydantic models to from JSON / YAML\n",
    "* Implementing JSON encoders for custom types\n",
    "* Fields and extending schema definitions\n",
    "* Config of serialization, excluding and including fields\n",
    "* Performance remarks for serialization\n",
    "\n",
    "In order to make use of Pydantic models we will need to get data in and/or out of instances of our models. This is known as deserialization and serialization, respectively. Pydantic was originally designed with the primary use case being web development where data is frequently serialized and deserialized in order to send and receive data between client and server. But Pydantic models can be useful in many other situations including, but not limited to, configuration files and data storage. The native Python serialization protocol is the pickle format, which is compatible with Pydantic, but pickling only works in a Python only system. If interfacing with other systems, pickling may not be possible. Furthermore, you may not always have control of the data files you need to validate. Common file formats used in the Python ecosystem are JSON (Javascript object notation), YAML (yet another markup language), and TOML (Tom's obvious, minimal language). Before we see examples of each of these, let's first create a function that takes in a serialization function and a deserialization function. The function will serialize a global data dict to string and print the string. It will then deserialize the string back to a Python object and print the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eefce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Optional\n",
    "\n",
    "data = {\n",
    "    \"data\": [0, 1, 1, 2, 3, 5],\n",
    "    \"attributes\": {\n",
    "        \"is_fibonacci\": True,\n",
    "        \"base_cases\": {\"f0\": 0, \"f1\": 1},\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def serialize_then_deserialize(\n",
    "    serialization_function: Callable[[Any], str],\n",
    "    deserialization_function: Callable[[str], Any],\n",
    "    serialization_kwargs: Optional[dict[str, Any]] = None,\n",
    "    deserialization_kwargs: Optional[dict[str, Any]] = None,\n",
    ") -> None:\n",
    "    if serialization_kwargs is None:\n",
    "        serialization_kwargs = {}\n",
    "    if deserialization_kwargs is None:\n",
    "        deserialization_kwargs = {}\n",
    "\n",
    "    data_serialized = serialization_function(\n",
    "        data,\n",
    "        **serialization_kwargs,\n",
    "    )\n",
    "    print(\"Serialized data to string:\")\n",
    "    print(data_serialized)\n",
    "\n",
    "    data_serialized_deserialized = deserialization_function(\n",
    "        data_serialized,\n",
    "        **deserialization_kwargs,\n",
    "    )\n",
    "    print(\"\\nDeserialized data from string:\")\n",
    "    print(data_serialized_deserialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8da1f3e",
   "metadata": {},
   "source": [
    "**JSON**\n",
    "\n",
    "Python has native JSON support in the standard library module `json`. Deserialization can be accomplished using `json.load` and `json.loads`, with the former taking a file pointed (an object a `.read()` method) and the latter taking a `str`, `bytes`, or `bytearray`. The analogous counterpoints (serialization) can be accomplished with `json.dump` and `json.dumps`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "serialize_then_deserialize(\n",
    "    serialization_function=json.dumps,\n",
    "    deserialization_function=json.loads,\n",
    "    serialization_kwargs={\"indent\": 2},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940da5f5",
   "metadata": {},
   "source": [
    "**YAML**\n",
    "\n",
    "Python does not natively support YAML files but third-party libraries exist such as PyYAML. Deserialization is accomplished with `yaml.load` and serialization is accomplished with `yaml.dump`. WARNING: The YAML specification is much more flexible than JSON and allows for execution of arbitrary Python functions. Thus it is recommended to use `yaml.load` only if your data comes from a trusted source. PyYAML also has `yaml.safe_load` and `yaml.safe_dump` that do not recognize arbitray Python objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880008f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "serialize_then_deserialize(\n",
    "    serialization_function=yaml.safe_dump,\n",
    "    deserialization_function=yaml.safe_load,\n",
    "    serialization_kwargs={\"sort_keys\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d734767c",
   "metadata": {},
   "source": [
    "**TOML**\n",
    "\n",
    "Starting with Python 3.11, Python does have native support for TOML files in the `tomllib` module. Earlier versions of python can use the `toml` third-party library. Like the `json` module, deserialization and serialization is accomplished with the `load`, `loads`, `dump`, and `dumps` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd3420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "\n",
    "serialize_then_deserialize(\n",
    "    serialization_function=toml.dumps,\n",
    "    deserialization_function=toml.loads,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca4a02",
   "metadata": {},
   "source": [
    "## Pydantic integration\n",
    "\n",
    "For deserialization Pydantic models have the `parse_raw`, `parse_obj`, and `parse_file` methods for `str`, `dict`, and `pathlib.Path` objects, respectively. Let's see each one in action by using the weather data in `my-data.json`. First we need to inspect the data and create a Pydantic model to represent the schema of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4357932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class TemperatureSample(BaseModel):\n",
    "    date: datetime.date\n",
    "    time: datetime.time\n",
    "    temperature: float\n",
    "\n",
    "\n",
    "class TemperatureData(BaseModel):\n",
    "    data: list[TemperatureSample]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961313e9",
   "metadata": {},
   "source": [
    "Let's start by reading the data in as a string and deserializing the string..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f5ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "fpath_temperature_data = Path.cwd() / \"my-data.json\"\n",
    "\n",
    "raw_temperature_data = fpath_temperature_data.read_text()\n",
    "print(\"Raw, unparsed, unvalidated data in string form:\")\n",
    "display(raw_temperature_data)\n",
    "\n",
    "temperature_data = TemperatureData.parse_raw(raw_temperature_data)\n",
    "print(\"\\nDeserialized data as a Pydantic model instance:\")\n",
    "display(temperature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afbfd97",
   "metadata": {},
   "source": [
    "The output is not so human friendly but we successfuly deserialized the raw JSON string into an instance of `TemperatureData`. Note that because the data is now in a `TemperatureData` instance, the data is also parsed and validated! Now what if we already had the data in memory as a Python object. Then we could use the `parse_obj` method..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_temperature_data_dict = json.loads(raw_temperature_data)\n",
    "print(\"Raw, unparsed, unvalidated data in dictionary form:\")\n",
    "display(raw_temperature_data_dict)\n",
    "\n",
    "temperature_data = TemperatureData.parse_obj(raw_temperature_data_dict)\n",
    "print(\"\\nDeserialized data as a Pydantic model instance:\")\n",
    "display(temperature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9477a0",
   "metadata": {},
   "source": [
    "Again we have deserialized, parsed, and validated the data into a `TemperatureData` instance. Finally, if we only have the path to a file containing the data, we can use the `parse_file` method..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd63921",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_data = TemperatureData.parse_file(fpath_temperature_data)\n",
    "print(\"Deserialized data as a Pydantic model instance:\")\n",
    "display(temperature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3ee9c",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* Currently pydantic only supports JSON and pickle files in the `parse_file` method. `pydantic-yaml` is an extension to Pydantic that provides this support, but a workaround is to load the data into memory and then use `parse_obj` or `parse_raw`.\n",
    "* `parse_obj` expects a dictionary, so other Python types cannot be used in this method\n",
    "\n",
    "The same temperature data exists in `my-data.yaml`, but it is a list of data points. Let's try this out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c97daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_temperature_data_yaml = Path.cwd() / \"my-data.yaml\"\n",
    "raw_temperature_data = yaml.safe_load(fpath_temperature_data_yaml.read_text())\n",
    "temperature_data = TemperatureData.parse_obj(\n",
    "    {\"data\": raw_temperature_data},\n",
    ")\n",
    "\n",
    "print(\"Deserialized data as a Pydantic model instance:\")\n",
    "display(temperature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbc4120",
   "metadata": {},
   "source": [
    "Now let's try serializing this data. Pydantic models have the `dict` and `json` methods to serialize to dictionaries and JSON strings, respectively. But first let's reduce the dataset to the first 3 entries so we dont flood our screen with a long list of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0ebd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_temperature_data = TemperatureData(data=temperature_data.data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983661e8",
   "metadata": {},
   "source": [
    "Now let's serialize the shortened dataset to a JSON string..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea3faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shortened_temperature_data.json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1dbf12",
   "metadata": {},
   "source": [
    "Once we have the serialized string, we could send the string to an API endpoint or write to disk. Now let's serialize to a Python dictionary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b35be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_temperature_data.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d9a072",
   "metadata": {},
   "source": [
    "Once again, we can then do with the serialized object as we please. For example, if we wanted to write to a TOML file, we could pass the serialized object into `toml.dump`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5cd82e",
   "metadata": {},
   "source": [
    "## Serialization of custom types\n",
    "\n",
    "What if we want to serialize a custom data type. Let's return to the `Point` class from Part-2-Basic-Usage..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa96811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    def __init__(self, x: float, y: float) -> None:\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(x={self.x}, y={self.y})\"\n",
    "\n",
    "    def __eq__(self, other: \"Point\") -> bool:\n",
    "        return (self.x == other.x) and (self.y == other.y)\n",
    "\n",
    "    def distance_to(self, other: \"Point\") -> float:\n",
    "        dx = self.x - other.x\n",
    "        dy = self.y - other.y\n",
    "        return (dx**2 + dy**2) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e87663a",
   "metadata": {},
   "source": [
    "If we build a Pydantic model that uses this class as a type hint, we won't be able to serialize using the `json` method..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34384186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineSegment(BaseModel):\n",
    "    p1: Point\n",
    "    p2: Point\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "\n",
    "line_segment = LineSegment(\n",
    "    p1=Point(x=1, y=3),\n",
    "    p2=Point(x=8, y=2),\n",
    ")\n",
    "\n",
    "line_segment.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce9e2c",
   "metadata": {},
   "source": [
    "We get an error message informing us the `Point` is not JSON serializable. We can fix this using the `json_encoders` attribute of the `Config` class. This attribute is a dictionary that maps field types to functions that serialize those types. So we can modify the `Point` class to include an instance method that will serialize the data in the `Point` instance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SerializablePoint(Point):\n",
    "    def serialize(self) -> dict[str, Any]:\n",
    "        return {\"x\": self.x, \"y\": self.y}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f8d1b6",
   "metadata": {},
   "source": [
    "Then we can modify our `LineSegment` model to include this JSON encoder..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2264327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineSegment(BaseModel):\n",
    "    p1: SerializablePoint\n",
    "    p2: SerializablePoint\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "        json_encoders = {\n",
    "            SerializablePoint: SerializablePoint.serialize,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eace403c",
   "metadata": {},
   "source": [
    "Now we can serialize an instance of `LineSegment`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a28573",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_segment = LineSegment(\n",
    "    p1=SerializablePoint(x=1, y=3),\n",
    "    p2=SerializablePoint(x=8, y=2),\n",
    ")\n",
    "\n",
    "line_segment.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b08756",
   "metadata": {},
   "source": [
    "We could have chosen to serialize `Point` in any number of reasonable ways. We chose to simply record the coordidates in a dictionary, but the possibilities are endless.\n",
    "\n",
    "What if we want to deserialize data for `Point` fields. Currently, this will not work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51131695",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_segment_data = {\"p1\": {\"x\": 1, \"y\": 3}, \"p2\": {\"x\": 8, \"y\": 2}}\n",
    "\n",
    "line_segment = LineSegment.parse_obj(line_segment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde5cf31",
   "metadata": {},
   "source": [
    "We need to add validators to the `SerializablePoint` class definition..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a057733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic.validators import float_validator\n",
    "\n",
    "\n",
    "class DeserializablePoint(SerializablePoint):\n",
    "    @classmethod\n",
    "    def __get_validators__(cls):\n",
    "        yield cls.deserialize\n",
    "\n",
    "    @classmethod\n",
    "    def deserialize(cls, data: dict[str, Any]) -> \"DeserializablePoint\":\n",
    "        if (\"x\" not in data) or (\"y\" not in data):\n",
    "            raise ValueError(\"Missing attributes x and/or y\")\n",
    "\n",
    "        x = float_validator(data[\"x\"])\n",
    "        y = float_validator(data[\"y\"])\n",
    "\n",
    "        return cls(x=data[\"x\"], y=data[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942db39a",
   "metadata": {},
   "source": [
    "Now we have a `Point` class that is both serializable and deserializable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c11b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineSegment(BaseModel):\n",
    "    p1: DeserializablePoint\n",
    "    p2: DeserializablePoint\n",
    "\n",
    "    class Config:\n",
    "        json_encoders = {\n",
    "            DeserializablePoint: DeserializablePoint.serialize,\n",
    "        }\n",
    "\n",
    "\n",
    "line_segment_data = {\"p1\": {\"x\": 1, \"y\": 3}, \"p2\": {\"x\": 8, \"y\": 2}}\n",
    "\n",
    "line_segment = LineSegment.parse_obj(line_segment_data)\n",
    "display(line_segment)\n",
    "\n",
    "line_segment.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf28727",
   "metadata": {},
   "source": [
    "For simplicity, I implemented our deserializer to assume the incoming data is a dictionary. A more flexible deserializer would require more logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87732d36",
   "metadata": {},
   "source": [
    "## Include and exclude\n",
    "\n",
    "Both `model.dict()` and `model.json()` have `include` and `exclude` parameters that specify which field to include or exclude when serializing model data. Other parameters exist as well, see [Exporting models](https://docs.pydantic.dev/latest/usage/exporting_models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38559d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(line_segment.json(indent=2, exclude={\"p1\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64fc6af",
   "metadata": {},
   "source": [
    "## Fields and extending schema definitions\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3cc9b",
   "metadata": {},
   "source": [
    "## `Config` related to serialization and deserialization\n",
    "\n",
    "Certain attributes in the `Config` class relate to serialization and deserialization. A description of a select few are:\n",
    "* `use_enum_values` - For `Enum` fields, the enumeration values will be used (as opposed to the Enum itself) when serializing with `model.dict()`.\n",
    "* `arbitrary_types_allowed` - Setting this to `True` allows the use of arbitrary types for fields (i.e. classes that do not define the `__get_validators__` method. This could be useful is you want to use `PIL.Image.Image` as a field type for example.\n",
    "* `json_loads` - A custom function for decoding JSON; see [custom JSON (de)serialisation](https://docs.pydantic.dev/latest/usage/exporting_models/#custom-json-deserialisation)\n",
    "* `json_dumps` - A custom function for encoding JSON; see [custom JSON (de)serialisation](https://docs.pydantic.dev/latest/usage/exporting_models/#custom-json-deserialisation)\n",
    "* `json_encoders` - A dict used to customise the way types are encoded to JSON; see [JSON Serialisation](https://docs.pydantic.dev/latest/usage/exporting_models/#modeljson)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888575b5",
   "metadata": {},
   "source": [
    "## Performance remarks for serialization\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b998e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
